{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7634b7dd",
   "metadata": {},
   "source": [
    "# How to Train a LEOPARD\n",
    "\n",
    "This notebook provides a brief instruction for training your own LEOPARD. You can also use the following code to reproduce the result of the MGH COVID dataset reported in our paper. The result may vary slightly with each run due to the stochastic mechanisms involved. Any questions regarding the code, please contact the zookeeper: Siyu Han (siyu.han@tum.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbbea7",
   "metadata": {},
   "source": [
    "## Step 1: Import required modules\n",
    "\n",
    "Before proceeding, ensure that the following dependencies are properly installed:\n",
    "- python: 3.79\n",
    "- numpy: 1.21.5\n",
    "- pandas: 1.3.5\n",
    "- scikit-learn: 1.0.2\n",
    "- pytorch: 1.11.0\n",
    "- pytorch_lightning: 1.6.4\n",
    "- tensorboard: 2.10.0\n",
    "- cuda (if use GPU): 11.3\n",
    "\n",
    "*The listed version numbers represent those utilized during our development. We cannot guarantee the compatibility or identical outcomes with different versions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ae2eb4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from src.data import *\n",
    "from src.train import TrainLEOPARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec8472",
   "metadata": {},
   "source": [
    "## Step 2: Prepare dataset\n",
    "\n",
    "This is done by the function `prepare_dataset()`. This function does the following things: \n",
    "1. load data and format them into training/validation/test sets with the function `load_split_data()`. \n",
    "2. scale data use a scaler with the function `scale_data()`. \n",
    "3. create an instance of `OmicsDataset` class using scaled data for each data split.\n",
    "\n",
    "The function `prepare_dataset()` receives arguments for the following parameters:\n",
    "- `data_dir`: a string to specify the folder containing the data of two views (A and B) and two timepoints (1 and 2). The data are saved in .csv files with names like \"vA_t1_test.csv\", \"vB_t1_train.csv\". \"v\" and \"t\" denote views and timepoints. \"v\\*_t\\*_train.csv\" used for model training and validation. Missing values can be encoded as NA. Even if data from view B at timepoint 2 are completely missing, you still need to provide a \"vB_t2_train.csv\" file with the corresponding sample ID and variable ID, and missing values are indicated with NA. \"v\\*_t\\*_test.csv\" is optional and only used for performance evaluation. Default: `\"data\\MGH_COVID\"`\n",
    "- `valSet_ratio`: a numeric value between 0 and 1 to specify the ratio of data from \"v\\*_t\\*_train.csv\" used for constructing the validation set. Default: `0.2`\n",
    "- `trainNum`: a numeric value or `\"all\"` indicating how many samples will be randomly selected from the training data for training. Default: `\"all\"`\n",
    "- `obsNum`: a numeric value or `\"all\"` indicating how many samples from \"vB_t2_train.csv\" will be used for training. Default: `0` \n",
    "- `use_scaler`: a string to indicate which scaler is used to scale data. Support `\"standard\"`, `\"robust\"`, and `\"minmax\"`. Description of the scalers please refer to the User Guide of `sklearn.preprocessing`. Default: `\"standard\"`\n",
    "- `set_seed`: a numeric value to set seed for reproducible results. Default: `1`\n",
    "- `save_data_dir`: `None` or a path used for saving the indices of samples used in training. Default: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09464be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ loading data:\n",
      "  -load vA_t1 as vA_t1\n",
      "  -load vA_t2 as vA_t2\n",
      "  -load vB_t1 as vB_t1\n",
      "  -load vB_t2 as vB_t2\n",
      "+ Data scaling using standard scaler:\n",
      "  - vA_t1: use scaler_vA\n",
      "  - vA_t2: use scaler_vA\n",
      "  - vB_t1: use scaler_vB\n",
      "  - vB_t2: use scaler_vB\n"
     ]
    }
   ],
   "source": [
    "obsNum = 0\n",
    "trainNum = \"all\"\n",
    "\n",
    "loaded_data = prepare_dataset(data_dir=\"data/MGH_COVID\", valSet_ratio=0.2, \n",
    "                              trainNum=trainNum, obsNum=obsNum, \n",
    "                              use_scaler=\"standard\", set_seed=1, save_data_dir=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb68b4",
   "metadata": {},
   "source": [
    "## Step 3: Create an instance of the `TrainLEOPARD` class\n",
    "\n",
    "The pytorch code of LEOPARD is organized into `TrainLEOPARD`, a `LightningModule`. LEOPARD is fully customizable. You can adapt your LEOPARD using the following parameters when instantiating an instance of the `TrainLEOPARD` class:\n",
    "- `train_set`, `val_set`, `test_set`: data prepared by `prepare_dataset()` for training, validation and test\n",
    "- `scaler_viewA`, `scaler_viewB`: scaler used by `prepare_dataset()` to transform data in two views\n",
    "- `pre_layers_viewA`, `pre_layers_viewB`: pre-layers for view A and view B to convert them to the same dimension. Default: `[64]`, `[64]`\n",
    "- `post_layers_viewA`, `post_layers_viewB`: post-layers for view A and view B to convert embeddings back to data in the original dimension. Default: `[64]`, `[64]`\n",
    "\n",
    "- `encoder_content_layers`: layers for the content encoder. A list where the length indicates the total number of layers, and each element specifies the size of the corresponding layer. Default: `[64, 64, 64]`\n",
    "- `encoder_content_norm`: a list indicates if using normalization for the layers in the content encoder. Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"instance\", \"instance\", \"instance\"]`\n",
    "- `encoder_content_dropout`: a list specifies dropout rate for each layer in the content encoder. Default: `[0, 0, 0]`\n",
    "    \n",
    "- `encoder_temporal_layers`: layers for the temporal encoder. Default: `[64, 64, 64]`\n",
    "- `encoder_temporal_norm`: if use normalization for the layers in the temporal encoder? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\", \"none\"]`\n",
    "- `encoder_temporal_dropout`: dropout rate for each layer in the temporal encoder. Default: `[0, 0, 0]`\n",
    "    \n",
    "- `generator_block_num`: how many layers/blocks are used for the generator. Default: `3`\n",
    "- `generator_norm`: if use normalization for the layers in the generator? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\", \"none\"]`\n",
    "- `generator_dropout`: dropout rate for each layer in the generator. Default: `[0, 0, 0]`\n",
    "- `merge_mode`: re-entangle content and temporal representations by concatenation (`\"concat\"`) or AdaIN (`\"adain\"`)? Default: `\"adain\"`\n",
    "    \n",
    "- `discriminator_layers`: layers for the multi-task discriminator. Default: `[128, 128]`\n",
    "- `discriminator_norm`: if use normalization for the layers in the discriminator? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\"]`\n",
    "- `discriminator_dropout`: dropout rate for each layer in the discriminator. Default: `[0, 0]`\n",
    "    \n",
    "- `reconstruction_loss`: use `\"MSE\"` or `\"MAE\"` to compute reconstruction loss? Default: `\"MSE\"`\n",
    "- `adversarial_loss`: use `\"MSE\"` or `\"BCE\"` to compute adversarial loss? Default: `\"MSE\"`\n",
    "- `weight_reconstruction`, `weight_adversarial`, `weight_representation`, `weight_contrastive`: weights for different losses. Default: `1`, `1`, `0.1`, `0.1`\n",
    "- `contrastive_temperature`: temperature for NT-Xent-based contrastive loss. Default: `0.05`\n",
    "    \n",
    "- `lr_G`, `lr_D`: learning rate for generator process (encoders and generator) and discrimination process (discriminator). ***You need to tune this for your own datasets.*** Default: `0.0005`, `0.0005`\n",
    "- `b1_G`, `b1_D`: beta_1 for Adam Optimizer. Default: `0.9`, `0.9`\n",
    "- `lr_scheduler_G`, `lr_scheduler_D`: `\"none\"` or use `\"LambdaLR\"` or `\"SGDR\"` as lr scheduler? Default: `\"none\"`, `\"none\"`\n",
    "    \n",
    "- `use_projection_head`: if use projection head for contrastive learning? Default: `False`\n",
    "- `projection_output_size`: set output size of projection head. Ignored if `use_projection_head=False`.  Default: `0`\n",
    "- `batch_size`: batch size. ***You need to adjust this based on your sample size.*** Default: `32`\n",
    "- `note`: add some additional texts as a hyperparameter to label each run.  Default: `\"obsNum_\" + str(obsNum)`\n",
    "\n",
    "*Some hyperparameters (especially `lr_G`, `lr_D`, and `batch_size`) may need to be tuned for your own datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a31b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_leopard = TrainLEOPARD(train_set=loaded_data['train_set'], val_set=loaded_data['val_set'],\n",
    "                          test_set=loaded_data['test_set'],\n",
    "                          scaler_viewA=loaded_data['scaler_viewA'], scaler_viewB=loaded_data['scaler_viewB'],\n",
    "                          \n",
    "                          pre_layers_viewA=[64], pre_layers_viewB=[64],\n",
    "                          post_layers_viewA=[64], post_layers_viewB=[64],\n",
    "                          \n",
    "                          encoder_content_layers=[64, 64, 64],\n",
    "                          encoder_content_norm=['instance', 'instance', 'instance'],\n",
    "                          encoder_content_dropout=[0, 0, 0],\n",
    "                          \n",
    "                          encoder_temporal_layers=[64, 64, 64],\n",
    "                          encoder_temporal_norm=['none', 'none', 'none'],\n",
    "                          encoder_temporal_dropout=[0, 0, 0],\n",
    "                          \n",
    "                          generator_block_num=3,\n",
    "                          generator_norm=['none', 'none', 'none'],\n",
    "                          generator_dropout=[0, 0, 0],\n",
    "                          merge_mode='adain',\n",
    "                          \n",
    "                          discriminator_layers=[64, 64],\n",
    "                          discriminator_norm=['none', 'none'],\n",
    "                          discriminator_dropout=[0, 0],\n",
    "                          \n",
    "                          reconstruction_loss='MSE', adversarial_loss='MSE',\n",
    "                          weight_reconstruction=1, weight_adversarial=1,\n",
    "                          weight_representation=0.1,\n",
    "                          weight_contrastive=0.1,\n",
    "                          contrastive_temperature=0.05,\n",
    "                          \n",
    "                          lr_G=0.0005, lr_D=0.0005, b1_G=0.9, b1_D=0.9,\n",
    "                          lr_scheduler_G='none', lr_scheduler_D='none',\n",
    "                          \n",
    "                          use_projection_head=False, projection_output_size=0,\n",
    "                          batch_size=32, note=\"obsNum_\" + str(obsNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d39f2",
   "metadata": {},
   "source": [
    "## Step 4: Create an instance of the `Trainer` class\n",
    "\n",
    "This is done by calling `Trainer()` from `pytorch_lightning`. `Trainer` can help you train your LEOPARD. Here we use the following settings (please refer to its Docs for a comprehensive parameter explanation):\n",
    "- `enable_progress_bar`: show progress bar? Default: `True`\n",
    "- `log_every_n_steps`: a numeric value that specifies the interval, in steps, at which metrics should be logged. Default: `3`\n",
    "- `max_epochs`: a numeric value that defines the maximum number of epochs the training loop should run. Default: `100`\n",
    "- `gpus`: a value indicating which GPUs to use. Default: `1 if torch.cuda.is_available() else None`\n",
    "- `logger`: a tensorboard logger responsible for logging training/validation metrics and other experiment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391d12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(\"lightning_logs\", \"trainNum_\" + str(trainNum))\n",
    "name = \"obsNum_\" + str(obsNum)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=3,\n",
    "    max_epochs=199,\n",
    "    gpus=1 if torch.cuda.is_available() else None,\n",
    "    logger=TensorBoardLogger(save_dir=save_dir, name=name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9c5fb",
   "metadata": {},
   "source": [
    "## Step 5: Train your LEOPARD\n",
    "\n",
    "Now let's train your LEOPARD!\n",
    "\n",
    "Optional: you can also visualize the training process with the logger. Use the `%tensorboard` magic command or call it in command line: `tensorboard --logdir *save_dir* --port 8080`\n",
    "(*use your own `save_dir` and port number*)\n",
    "\n",
    "In tensorboard, you can monitor the losses computed on the training set and validation set (if you have one), which can help mitigate the risk of overfitting. For example, you might want to stop the training process if the reconstruction loss starts to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: invoke TensorBoard with the %tensorboard magic command.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d32d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs\\trainNum_all\\obsNum_0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                              | Type    | Params\n",
      "--------------------------------------------------------------\n",
      "0 | loss_F_reconstruction_noReduction | MSELoss | 0     \n",
      "1 | loss_F_reconstruction             | MSELoss | 0     \n",
      "2 | loss_F_adversarial                | MSELoss | 0     \n",
      "3 | leopard                           | LEOPARD | 134 K \n",
      "--------------------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.538     Total estimated model params size (MB)\n",
      "C:\\Users\\SiyuHan\\anaconda3\\envs\\LEOPARD\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "C:\\Users\\SiyuHan\\anaconda3\\envs\\LEOPARD\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    }
   ],
   "source": [
    "# train a LEOPARD\n",
    "\n",
    "trainer.fit(my_leopard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9b15",
   "metadata": {},
   "source": [
    "## Step 6: Impute and export data\n",
    "\n",
    "You can impute the missing data and writing them into a .csv file. If you have ground truth for \"vB_t2_test.csv\", you can also export percent bias of the imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ec9f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\SiyuHan\\anaconda3\\envs\\LEOPARD\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:245: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    }
   ],
   "source": [
    "# impute data\n",
    "imputed_data = trainer.predict(my_leopard, my_leopard.test_dataloader())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a2d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for output\n",
    "output_dir = os.path.join(save_dir, name, \"version_\" + str(trainer.logger.version), \"results\") \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# export imputed data\n",
    "pd.DataFrame(imputed_data[\"generated_data\"]).to_csv(\n",
    "            os.path.join(output_dir, \"imputedData_obs\" + str(obsNum) + \".csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6746e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export percent bias (only when groundtruth is available)\n",
    "pd.DataFrame(imputed_data[\"raw_percentBias\"]).to_csv(\n",
    "            os.path.join(output_dir, \"PB_obs\" + str(obsNum) + \".csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3cdeb",
   "metadata": {},
   "source": [
    "## End\n",
    "This manual is prepared based on our analysis and has been tested on our benchmark datasets. \n",
    "Please let us know if you found any issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
