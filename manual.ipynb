{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7634b7dd",
   "metadata": {},
   "source": [
    "# How to Train a LEOPARD\n",
    "\n",
    "This notebook provides a brief instruction for training your own LEOPARD. You can also use the following code to reproduce the result of the MGH COVID dataset reported in our paper. The result may vary slightly with each run due to the stochastic mechanisms involved. Any questions regarding the code, please contact the zookeeper: Siyu Han (siyu.han@tum.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbbea7",
   "metadata": {},
   "source": [
    "## Step 1: Import required modules\n",
    "\n",
    "Before proceeding, ensure that the following dependencies are properly installed:\n",
    "- python: 3.79\n",
    "- numpy: 1.21.5\n",
    "- pandas: 1.3.5\n",
    "- scikit-learn: 1.0.2\n",
    "- pytorch: 1.11.0\n",
    "- pytorch_lightning: 1.6.4\n",
    "- tensorboard: 2.10.0\n",
    "- cuda (if use GPU): 11.3\n",
    "\n",
    "*The listed version numbers represent those utilized during our development. We cannot guarantee the compatibility or identical outcomes with different versions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae2eb4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from src.data import *\n",
    "from src.train import TrainLEOPARD\n",
    "\n",
    "print(\"current pytorch version: \", torch.__version__)         # 1.11.0\n",
    "print(\"current pytorch_lightning version: \", pl.__version__)  # 1.6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec8472",
   "metadata": {},
   "source": [
    "## Step 2: Prepare dataset\n",
    "\n",
    "This is done by the function `prepare_dataset()`. This function does the following things: \n",
    "1. load data and format them into training/validation/test sets with the function `load_split_data()`. \n",
    "2. scale data use a scaler with the function `scale_data()`. \n",
    "3. create an instance of `OmicsDataset` class using scaled data for each data split.\n",
    "\n",
    "The function `prepare_dataset()` receives arguments for the following parameters:\n",
    "- `data_dir`: a string to specify the folder containing the data of complete views (vA) and incomplete views (vB) and multiple timepoints (1, 2, ...). The input data are saved in .csv files with names like \"vA_train.csv\", \"vB_train.csv\". \"v\" and \"t\" denote views and timepoints. The incomplete views in vB will be imputed. \"v\\*_t\\*_train.csv\" used for model training and validation. Missing values can be encoded as NA. Even if data from view B at a specific timepoint are completely missing, you still need to ensure the file contains the corresponding sample ID and variable ID, where missing views are indicated with NA. \"v\\*_test.csv\" is only used for performance evaluation where data cannot be seen by the model during training. If you only want to complete missing views in your dataset, you can save the same data into \"v\\*_train.csv\" and \"v\\*_test.csv\" to get imputed result. Default: `\"data\\MGH_COVID\"`\n",
    "- `missTimepoint`: a string to specify which timepoint is missing and needs to be completed. Default: `\"D3\"`\n",
    "- `valSet_ratio`: a numeric value between 0 and 1 to specify the ratio of data from \"v\\*_t\\*_train.csv\" used for constructing the validation set. Default: `0.2`\n",
    "- `trainNum`: a numeric value or `\"all\"` indicating how many samples will be randomly selected from the training data for training. Default: `\"all\"`\n",
    "- `obsNum`: a numeric value or `\"all\"` indicating how many samples from \"vB_t2_train.csv\" will be used for training. Default: `0` \n",
    "- `use_scaler`: a string to indicate which scaler is used to scale data. Support `\"standard\"`, `\"robust\"`, and `\"minmax\"`. Description of the scalers please refer to the User Guide of `sklearn.preprocessing`. Default: `\"standard\"`\n",
    "- `save_data_dir`: `None` or a path used for saving the indices of samples used in training. Default: `None`\n",
    "- `set_seed`: a numeric value to set seed for reproducible results. Default: `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09464be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obsNum = 0\n",
    "trainNum = \"all\"\n",
    "\n",
    "loaded_data = prepare_dataset(data_dir=\"data/MGH_COVID\", missTimepoint=\"D3\",\n",
    "                              valSet_ratio=0.2, trainNum=trainNum, obsNum=obsNum, \n",
    "                              use_scaler=\"standard\", save_data_dir=None, set_seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb68b4",
   "metadata": {},
   "source": [
    "## Step 3: Create an instance of the `TrainLEOPARD` class\n",
    "\n",
    "The pytorch code of LEOPARD is organized into `TrainLEOPARD`, a `LightningModule`. LEOPARD is fully customizable. You can adapt your LEOPARD using the following parameters when instantiating an instance of the `TrainLEOPARD` class:\n",
    "\n",
    "#### arugments for input data:\n",
    "- `loaded_data`: output of `prepare_dataset()`, including prepared data splits and scalers\n",
    "- `pre_layers_viewA`, `pre_layers_viewB`: pre-layers for view A and view B to convert them to the same dimension. Default: `[64]`, `[64]`\n",
    "- `post_layers_viewA`, `post_layers_viewB`: post-layers for view A and view B to convert embeddings back to data in the original dimension. Default: `[64]`, `[64]`\n",
    "\n",
    "#### arugments for the content encoder:\n",
    "- `encoder_content_layers`: layers for the content encoder. A list where the length indicates the total number of layers, and each element specifies the size of the corresponding layer. Default: `[64, 64, 64]`\n",
    "- `encoder_content_norm`: a list indicates if using normalization for the layers in the content encoder. Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"instance\", \"instance\", \"instance\"]`\n",
    "- `encoder_content_dropout`: a list specifies dropout rate for each layer in the content encoder. Default: `[0, 0, 0]`\n",
    "\n",
    "#### arugments for the temporal encoder:\n",
    "- `encoder_temporal_layers`: layers for the temporal encoder. Default: `[64, 64, 64]`\n",
    "- `encoder_temporal_norm`: if use normalization for the layers in the temporal encoder? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\", \"none\"]`\n",
    "- `encoder_temporal_dropout`: dropout rate for each layer in the temporal encoder. Default: `[0, 0, 0]`\n",
    "\n",
    "#### arugments for the projection head:\n",
    "- `use_projection_head`: if use projection head for contrastive learning? Default: `False`\n",
    "- `projection_output_size`: set output size of projection head. Ignored if `use_projection_head=False`.  Default: `0`\n",
    "\n",
    "#### arugments for the generator:\n",
    "- `generator_block_num`: how many layers/blocks are used for the generator. Default: `3`\n",
    "- `generator_norm`: if use normalization for the layers in the generator? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\", \"none\"]`\n",
    "- `generator_dropout`: dropout rate for each layer in the generator. Default: `[0, 0, 0]`\n",
    "- `merge_mode`: re-entangle content and temporal representations by concatenation (`\"concat\"`) or AdaIN (`\"adain\"`)? Default: `\"adain\"`\n",
    "\n",
    "#### arugments for the discriminator:\n",
    "- `discriminator_layers`: layers for the multi-task discriminator. Default: `[128, 128]`\n",
    "- `discriminator_norm`: if use normalization for the layers in the discriminator? Supported `\"instance\"`, `\"batch\"`, and `\"none\"`. Default: `[\"none\", \"none\"]`\n",
    "- `discriminator_dropout`: dropout rate for each layer in the discriminator. Default: `[0, 0]`\n",
    "\n",
    "#### arugments for configuring losses:\n",
    "- `reconstruction_loss`: use `\"MSE\"` or `\"MAE\"` to compute reconstruction loss? Default: `\"MSE\"`\n",
    "- `adversarial_loss`: use `\"MSE\"` or `\"BCE\"` to compute adversarial loss? Default: `\"MSE\"`\n",
    "- `weight_reconstruction`, `weight_adversarial`, `weight_representation`, `weight_contrastive`: weights for different losses. Default: `1`, `1`, `0.1`, `0.1`\n",
    "- `contrastive_temperature`: temperature for NT-Xent-based contrastive loss. Default: `0.05`\n",
    "\n",
    "#### arugments for optimization:\n",
    "- `lr_G`, `lr_D`: learning rate for generator process (encoders and generator) and discrimination process (discriminator). ***You need to tune this for your own datasets.*** Default: `0.0005`, `0.0005`\n",
    "- `b1_G`, `b1_D`: beta_1 for Adam Optimizer. Default: `0.9`, `0.9`\n",
    "- `lr_scheduler_G`, `lr_scheduler_D`: `\"none\"` or use `\"LambdaLR\"` or `\"SGDR\"` as lr scheduler? Default: `\"none\"`, `\"none\"`\n",
    "- `batch_size`: batch size. ***You need to adjust this based on your sample size.*** Default: `32`\n",
    "\n",
    "#### additional arguments:\n",
    "- `save_embedding_dir`: `None` or a path used for saving the content and temporal embeddings. \n",
    "- `save_embedding_every_n_epoch`: an integar to indicate how often to save embeddings. Ignored when `output_embedding=None`.  \n",
    "- `note`: add some additional texts as a hyperparameter to label each run.  Default: `\"obsNum_\" + str(obsNum)`\n",
    "\n",
    "*Some hyperparameters (especially `lr_G`, `lr_D`, and `batch_size`) may need to be tuned for your own datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_leopard = TrainLEOPARD(loaded_data=loaded_data,\n",
    "                          pre_layers_viewA=[64], pre_layers_viewB=[64],\n",
    "                          post_layers_viewA=[64], post_layers_viewB=[64],\n",
    "                          \n",
    "                          encoder_content_layers=[64, 64, 64],\n",
    "                          encoder_content_norm=['instance', 'instance', 'instance'],\n",
    "                          encoder_content_dropout=[0, 0, 0],\n",
    "                          \n",
    "                          encoder_temporal_layers=[64, 64, 64],\n",
    "                          encoder_temporal_norm=['none', 'none', 'none'],\n",
    "                          encoder_temporal_dropout=[0, 0, 0],\n",
    "                          \n",
    "                          use_projection_head=False, \n",
    "                          projection_output_size=0,\n",
    "                          \n",
    "                          generator_block_num=3,\n",
    "                          generator_norm=['none', 'none', 'none'],\n",
    "                          generator_dropout=[0, 0, 0],\n",
    "                          merge_mode='adain',\n",
    "                          \n",
    "                          discriminator_layers=[64, 64],\n",
    "                          discriminator_norm=['none', 'none'],\n",
    "                          discriminator_dropout=[0, 0],\n",
    "                          \n",
    "                          reconstruction_loss='MSE', adversarial_loss='MSE',\n",
    "                          weight_reconstruction=1, weight_adversarial=1,\n",
    "                          weight_representation=0.1,\n",
    "                          weight_contrastive=0.1,\n",
    "                          contrastive_temperature=0.05,\n",
    "                          \n",
    "                          lr_G=0.0005, lr_D=0.0005, b1_G=0.9, b1_D=0.9,\n",
    "                          lr_scheduler_G='none', lr_scheduler_D='none',\n",
    "                          batch_size=32, \n",
    "                           \n",
    "                          save_embedding_dir=os.path.join(\"lightning_logs\", \"trainNum_\" + str(trainNum), \n",
    "                                                          \"obsNum_\" + str(obsNum),\n",
    "                                                          \"disentangled_embeddings\"),\n",
    "                          save_embedding_every_n_epoch=10,\n",
    "                          note=\"obsNum_\" + str(obsNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d39f2",
   "metadata": {},
   "source": [
    "## Step 4: Create an instance of the `Trainer` class\n",
    "\n",
    "This is done by calling `Trainer()` from `pytorch_lightning`. `Trainer` can help you train your LEOPARD. Here we use the following settings (please refer to its Docs for a comprehensive parameter explanation):\n",
    "- `enable_progress_bar`: show progress bar? Default: `True`\n",
    "- `log_every_n_steps`: a numeric value that specifies the interval, in steps, at which metrics should be logged. Default: `3`\n",
    "- `max_epochs`: a numeric value that defines the maximum number of epochs the training loop should run. Default: `100`\n",
    "- `gpus`: a value indicating which GPUs to use. Default: `1 if torch.cuda.is_available() else None`\n",
    "- `logger`: a tensorboard logger responsible for logging training/validation metrics and other experiment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"lightning_logs\", \"trainNum_\" + str(trainNum))\n",
    "name = \"obsNum_\" + str(obsNum)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=3,\n",
    "    max_epochs=199,\n",
    "    gpus=1 if torch.cuda.is_available() else None,\n",
    "    logger=TensorBoardLogger(save_dir=save_dir, name=name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9c5fb",
   "metadata": {},
   "source": [
    "## Step 5: Train your LEOPARD\n",
    "\n",
    "Now let's train your LEOPARD!\n",
    "\n",
    "Optional: you can also visualize the training process with the logger. Use the `%tensorboard` magic command or call it in command line: `tensorboard --logdir *save_dir* --port 8080`\n",
    "(*use your own `save_dir` and port number*)\n",
    "\n",
    "In tensorboard, you can monitor the losses computed on the training set and validation set (if you have one), which can help mitigate the risk of overfitting. For example, you might want to stop the training process if the reconstruction loss starts to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: invoke TensorBoard with the %tensorboard magic command.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LEOPARD\n",
    "\n",
    "trainer.fit(my_leopard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9b15",
   "metadata": {},
   "source": [
    "## Step 6: Impute and export data\n",
    "\n",
    "You can impute the missing data and writing them into a .csv file. If you have ground truth for \"vB_t2_test.csv\", you can also export percent bias of the imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute data\n",
    "imputed_data = trainer.predict(my_leopard, my_leopard.test_dataloader())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36195cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for output\n",
    "output_dir = os.path.join(save_dir, name, \"version_\" + str(trainer.logger.version), \"results\") \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# export imputed data\n",
    "pd.DataFrame(imputed_data[\"generated_data\"]).to_csv(\n",
    "            os.path.join(output_dir, \"imputedData_obs\" + str(obsNum) + \".csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6746e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export percent bias (only when groundtruth is available)\n",
    "pd.DataFrame(imputed_data[\"raw_percentBias\"]).to_csv(\n",
    "            os.path.join(output_dir, \"PB_obs\" + str(obsNum) + \".csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3cdeb",
   "metadata": {},
   "source": [
    "## End\n",
    "This manual is prepared based on our analysis and has been tested on our benchmark datasets. \n",
    "Please let us know if you found any issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
